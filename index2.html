<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ðŸ§  Neo AI Face</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Comic Sans MS', cursive, sans-serif;
            background: linear-gradient(45deg, #667eea, #764ba2, #f093fb, #f5576c);
            background-size: 400% 400%;
            animation: gradientShift 15s ease infinite;
            width: 100vw;
            height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            overflow: hidden;
        }

        @keyframes gradientShift {
            0% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
            100% { background-position: 0% 50%; }
        }

        /* Neo's Face - Full Screen */
        .neo-face-container {
            position: relative;
            width: 480px;
            height: 320px;
            background: linear-gradient(145deg, #f0f8ff, #e6f3ff);
            border-radius: 40px;
            box-shadow: 
                inset 0 20px 40px rgba(255, 255, 255, 0.5),
                inset 0 -20px 40px rgba(0, 0, 0, 0.1),
                0 20px 60px rgba(0, 0, 0, 0.3);
            display: flex;
            align-items: center;
            justify-content: center;
            border: 8px solid rgba(255, 255, 255, 0.8);
            animation: float 6s ease-in-out infinite;
        }

        @keyframes float {
            0%, 100% { transform: translateY(0px) rotate(0deg); }
            50% { transform: translateY(-10px) rotate(1deg); }
        }

        .face {
            position: relative;
            width: 400px;
            height: 280px;
        }

        /* Eyes - Large and Expressive */
        .eyes {
            position: absolute;
            top: 80px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 100px;
        }

        .eye {
            width: 60px;
            height: 60px;
            background: #333;
            border-radius: 50%;
            position: relative;
            animation: blink 4s infinite;
            box-shadow: 
                inset 0 5px 10px rgba(0, 0, 0, 0.3),
                0 5px 15px rgba(0, 0, 0, 0.2);
        }

        .eye::after {
            content: '';
            position: absolute;
            top: 15px;
            left: 15px;
            width: 20px;
            height: 20px;
            background: white;
            border-radius: 50%;
            animation: eyeShine 3s ease-in-out infinite;
        }

        .eye::before {
            content: '';
            position: absolute;
            top: 25px;
            left: 25px;
            width: 8px;
            height: 8px;
            background: rgba(255, 255, 255, 0.8);
            border-radius: 50%;
        }

        @keyframes blink {
            0%, 45%, 55%, 100% { transform: scaleY(1); }
            50% { transform: scaleY(0.1); }
        }

        @keyframes eyeShine {
            0%, 100% { opacity: 0.8; }
            50% { opacity: 1; transform: scale(1.2); }
        }

        /* Listening State Eyes */
        .eyes.listening .eye {
            animation: listenBlink 2s infinite;
            background: #2ecc71;
        }

        @keyframes listenBlink {
            0%, 40%, 60%, 100% { transform: scaleY(1); }
            50% { transform: scaleY(0.8); }
        }

        /* Mouth - Large and Animated */
        .mouth {
            position: absolute;
            bottom: 60px;
            left: 50%;
            transform: translateX(-50%);
            width: 120px;
            height: 60px;
            border: 8px solid #ff6b6b;
            border-top: none;
            border-radius: 0 0 120px 120px;
            background: #ffb3ba;
            transition: all 0.3s ease;
            box-shadow: 
                inset 0 -10px 20px rgba(255, 107, 107, 0.3),
                0 10px 20px rgba(0, 0, 0, 0.1);
        }

        .mouth.talking {
            animation: talk 0.2s ease-in-out infinite alternate;
            background: #ff8a95;
            border-color: #ff4757;
        }

        .mouth.listening {
            border-radius: 60px;
            border-color: #2ecc71;
            background: #a8e6cf;
            width: 100px;
            height: 50px;
            animation: listenPulse 2s ease-in-out infinite;
        }

        .mouth.happy {
            border-radius: 0 0 120px 120px;
            border-color: #4ecdc4;
            background: #a8e6cf;
            animation: smile 3s ease-in-out infinite;
        }

        @keyframes talk {
            0% { 
                height: 40px; 
                border-radius: 0 0 80px 80px;
                transform: translateX(-50%) scaleX(0.9);
            }
            100% { 
                height: 80px; 
                border-radius: 0 0 140px 140px;
                transform: translateX(-50%) scaleX(1.1);
            }
        }

        @keyframes listenPulse {
            0%, 100% { transform: translateX(-50%) scale(1); opacity: 0.8; }
            50% { transform: translateX(-50%) scale(1.1); opacity: 1; }
        }

        @keyframes smile {
            0%, 100% { transform: translateX(-50%) scale(1); }
            50% { transform: translateX(-50%) scale(1.05); }
        }

        /* Status indicator - minimal */
        .status-dot {
            position: absolute;
            top: 20px;
            right: 20px;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: #2ecc71;
            animation: statusPulse 2s ease-in-out infinite;
            box-shadow: 0 0 20px rgba(46, 204, 113, 0.5);
        }

        .status-dot.listening {
            background: #f39c12;
            box-shadow: 0 0 20px rgba(243, 156, 18, 0.5);
            animation: listeningPulse 1s ease-in-out infinite;
        }

        .status-dot.speaking {
            background: #e74c3c;
            box-shadow: 0 0 20px rgba(231, 76, 60, 0.5);
            animation: speakingPulse 0.5s ease-in-out infinite;
        }

        @keyframes statusPulse {
            0%, 100% { transform: scale(1); opacity: 0.8; }
            50% { transform: scale(1.2); opacity: 1; }
        }

        @keyframes listeningPulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.5); }
        }

        @keyframes speakingPulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.3); }
        }

        /* Wake word detection indicator */
        .wake-indicator {
            position: absolute;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0, 0, 0, 0.7);
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 14px;
            font-weight: bold;
            opacity: 0;
            transition: opacity 0.3s ease;
        }

        .wake-indicator.show {
            opacity: 1;
            animation: fadeInOut 2s ease-in-out;
        }

        @keyframes fadeInOut {
            0%, 100% { opacity: 0; transform: translateX(-50%) translateY(10px); }
            50% { opacity: 1; transform: translateX(-50%) translateY(0); }
        }

        /* Audio visualization */
        .audio-visualizer {
            position: absolute;
            bottom: 10px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 3px;
            height: 30px;
            align-items: end;
            opacity: 0;
            transition: opacity 0.3s ease;
        }

        .audio-visualizer.active {
            opacity: 1;
        }

        .audio-bar {
            width: 4px;
            background: linear-gradient(to top, #667eea, #764ba2);
            border-radius: 2px;
            animation: audioWave 1s ease-in-out infinite;
        }

        .audio-bar:nth-child(1) { animation-delay: 0s; }
        .audio-bar:nth-child(2) { animation-delay: 0.1s; }
        .audio-bar:nth-child(3) { animation-delay: 0.2s; }
        .audio-bar:nth-child(4) { animation-delay: 0.3s; }
        .audio-bar:nth-child(5) { animation-delay: 0.4s; }
        .audio-bar:nth-child(6) { animation-delay: 0.2s; }
        .audio-bar:nth-child(7) { animation-delay: 0.1s; }

        @keyframes audioWave {
            0%, 100% { height: 5px; }
            50% { height: 25px; }
        }
    </style>
</head>
<body>
    <!-- Neo's Full Screen Face -->
    <div class="neo-face-container">
        <div class="status-dot" id="statusDot"></div>
        <div class="face">
            <div class="eyes" id="neoEyes">
                <div class="eye"></div>
                <div class="eye"></div>
            </div>
            <div class="mouth happy" id="neoMouth"></div>
        </div>
        <div class="wake-indicator" id="wakeIndicator">Listening for "Hey Neo"...</div>
        <div class="audio-visualizer" id="audioVisualizer">
            <div class="audio-bar"></div>
            <div class="audio-bar"></div>
            <div class="audio-bar"></div>
            <div class="audio-bar"></div>
            <div class="audio-bar"></div>
            <div class="audio-bar"></div>
            <div class="audio-bar"></div>
        </div>
    </div>

    <script>
        // Configuration
        const API_BASE = 'https://6423-197-25-169-35.ngrok-free.app';
        
        let isListening = false;
        let isRecording = false;
        let mediaRecorder;
        let audioChunks = [];
        let recognition;
        let currentEmotion = 'happy';

        // Initialize everything
        document.addEventListener('DOMContentLoaded', function() {
            initializeSpeechRecognition();
            startContinuousListening();
            setEmotion('happy');
            
            // Show initial wake indicator
            setTimeout(() => {
                showWakeIndicator();
            }, 2000);
        });

        // Speech Recognition Setup
        function initializeSpeechRecognition() {
            if ('webkitSpeechRecognition' in window) {
                recognition = new webkitSpeechRecognition();
                recognition.continuous = true;
                recognition.interimResults = true;
                recognition.lang = 'en-US';

                recognition.onresult = function(event) {
                    const transcript = event.results[event.results.length - 1][0].transcript.toLowerCase().trim();
                    console.log('Heard:', transcript);
                    
                    if (transcript.includes('hey neo') || transcript.includes('hi neo')) {
                        handleWakeWord();
                    }
                };

                recognition.onerror = function(event) {
                    console.log('Speech recognition error:', event.error);
                    setTimeout(startContinuousListening, 2000);
                };

                recognition.onend = function() {
                    console.log('Speech recognition ended, restarting...');
                    setTimeout(startContinuousListening, 1000);
                };
            } else {
                console.log('Speech recognition not supported, falling back to continuous recording');
                startContinuousRecording();
            }
        }

        function startContinuousListening() {
            if (recognition && !isListening) {
                try {
                    recognition.start();
                    isListening = true;
                    setEmotion('listening');
                    setStatus('listening');
                    console.log('ðŸ‘‚ Neo is listening for "Hey Neo"...');
                } catch (error) {
                    console.log('Speech recognition start error:', error);
                    setTimeout(startContinuousListening, 2000);
                }
            }
        }

        function handleWakeWord() {
            console.log('ðŸŽ‰ Wake word detected! Neo says: "Yes, I hear you!"');
            
            // Stop listening temporarily
            if (recognition) {
                recognition.stop();
                isListening = false;
            }
            
            // Neo acknowledges
            setEmotion('excited');
            setStatus('speaking');
            showWakeIndicator('Yes, I hear you!');
            
            // Speak acknowledgment
            speak("Yes, I hear you!");
            
            // Start recording user's message
            setTimeout(() => {
                startRecordingUserMessage();
            }, 2000);
        }

        function speak(text) {
            if ('speechSynthesis' in window) {
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.rate = 1.1;
                utterance.pitch = 1.2;
                utterance.volume = 0.9;
                
                utterance.onstart = () => {
                    setEmotion('talking');
                    setStatus('speaking');
                };
                
                utterance.onend = () => {
                    setEmotion('happy');
                    setStatus('ready');
                };
                
                speechSynthesis.speak(utterance);
            }
        }

        // Recording for user message after wake word
        async function startRecordingUserMessage() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                audioChunks = [];

                mediaRecorder.ondataavailable = event => {
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    await sendVoiceMessage(audioBlob);
                    stream.getTracks().forEach(track => track.stop());
                };

                mediaRecorder.start();
                isRecording = true;
                setEmotion('listening');
                setStatus('listening');
                showAudioVisualizer(true);
                showWakeIndicator('Listening to your message...');

                // Stop recording after 10 seconds or on silence
                setTimeout(() => {
                    if (isRecording) {
                        stopRecordingUserMessage();
                    }
                }, 10000);

            } catch (error) {
                console.error('Recording error:', error);
                restartListening();
            }
        }

        function stopRecordingUserMessage() {
            if (mediaRecorder && isRecording) {
                mediaRecorder.stop();
                isRecording = false;
                setEmotion('thinking');
                setStatus('processing');
                showAudioVisualizer(false);
                showWakeIndicator('Processing your message...');
            }
        }

        // Send voice message to server
        async function sendVoiceMessage(audioBlob) {
            try {
                const formData = new FormData();
                formData.append('audio', audioBlob, 'recording.wav');
                formData.append('child_name', 'friend');

                const response = await fetch(`${API_BASE}/chat/voice`, {
                    method: 'POST',
                    headers: {
                        'ngrok-skip-browser-warning': 'true'
                    },
                    body: formData
                });

                if (response.ok) {
                    const audioResponse = await response.blob();
                    const audioUrl = URL.createObjectURL(audioResponse);
                    
                    // Play Neo's response immediately
                    playNeoResponse(audioUrl);
                    
                } else {
                    throw new Error(`Server error: ${response.status}`);
                }
            } catch (error) {
                console.error('Voice message error:', error);
                speak("Sorry, I had trouble understanding. Please try again.");
                restartListening();
            }
        }

        function playNeoResponse(audioUrl) {
            const audio = new Audio(audioUrl);
            
            audio.onloadstart = () => {
                setEmotion('thinking');
                setStatus('processing');
            };
            
            audio.onplay = () => {
                setEmotion('talking');
                setStatus('speaking');
                showWakeIndicator('Neo is responding...');
            };
            
            audio.onended = () => {
                setEmotion('happy');
                restartListening();
            };
            
            audio.onerror = () => {
                speak("I had trouble with my voice. Let me try again.");
                restartListening();
            };
            
            // Auto-play the response
            audio.play().catch((error) => {
                console.log('Auto-play prevented:', error);
                restartListening();
            });
        }

        function restartListening() {
            setTimeout(() => {
                setEmotion('listening');
                setStatus('listening');
                showWakeIndicator();
                startContinuousListening();
            }, 2000);
        }

        // Emotion and UI Management
        function setEmotion(emotion) {
            currentEmotion = emotion;
            const mouth = document.getElementById('neoMouth');
            const eyes = document.getElementById('neoEyes');
            
            // Remove all emotion classes
            mouth.className = 'mouth';
            eyes.className = 'eyes';
            
            // Add new emotion class
            if (emotion !== 'neutral') {
                mouth.classList.add(emotion);
                if (emotion === 'listening') {
                    eyes.classList.add('listening');
                }
            }
        }

        function setStatus(status) {
            const statusDot = document.getElementById('statusDot');
            statusDot.className = `status-dot ${status}`;
        }

        function showWakeIndicator(message = 'Listening for "Hey Neo"...') {
            const indicator = document.getElementById('wakeIndicator');
            indicator.textContent = message;
            indicator.classList.add('show');
            
            setTimeout(() => {
                indicator.classList.remove('show');
            }, 3000);
        }

        function showAudioVisualizer(show) {
            const visualizer = document.getElementById('audioVisualizer');
            if (show) {
                visualizer.classList.add('active');
            } else {
                visualizer.classList.remove('active');
            }
        }

        // Fallback: Continuous recording if speech recognition fails
        function startContinuousRecording() {
            console.log('Using continuous recording mode');
            // Implement continuous recording with silence detection
            // This would be more complex but provides fallback
        }

        // Error handling and recovery
        window.addEventListener('error', function(e) {
            console.error('Global error:', e);
            restartListening();
        });

        // Keep the page alive
        setInterval(() => {
            console.log('ðŸ¤– Neo is alive and listening...');
        }, 30000);
    </script>
</body>
</html>